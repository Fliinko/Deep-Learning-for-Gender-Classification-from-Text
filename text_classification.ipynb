{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import nltk\n",
    "import torch \n",
    "import sklearn \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender classification assignment\n",
    "\n",
    "You are to follow the instructions below and fill each cell as instructed.\n",
    "Once ready, submit this notebook on VLE with all the outputs included (run all your code and don't clear any output cells).\n",
    "Do not submit anything else apart from the notebook and do not use any extra data apart from what is provided.\n",
    "\n",
    "You will be working on classifying the genders of people from their blog posts using a data set called the [Blog Authorship Corpus](https://www.kaggle.com/rtatman/blog-authorship-corpus).\n",
    "This has been pre-split and reduced for you to use in this assignment.\n",
    "\n",
    "10% of the marks from this assignment are based on neatness.\n",
    "\n",
    "This assignment will carry 40% of the final mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing (10%)\n",
    "\n",
    "You have a train/dev/test split data set consisting of CSV files with two fields: gender and text.\n",
    "The gender field contains either 'male' or 'female' whilst the text is a string containing text from blog posts.\n",
    "\n",
    "Do the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load these three CSV files and tokenise each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolower(data):\n",
    "    return data.lower()\n",
    "\n",
    "#Import\n",
    "dev = pd.read_csv(\"dev.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#Split to X and Y \n",
    "dev_x = dev['text']\n",
    "dev_y = dev['gender']\n",
    "\n",
    "test_x = test['text']\n",
    "test_y = test['gender']\n",
    "\n",
    "train_x = train['text']\n",
    "train_y = train['gender']\n",
    "\n",
    "#Convert to Lower Case\n",
    "dev_x = dev_x.apply(tolower)\n",
    "test_x = test_x.apply(tolower)\n",
    "train_x = train_x.apply(tolower)\n",
    "\n",
    "#Tokenize Text \n",
    "tdev_x = dev_x.apply(nltk.tokenize.word_tokenize)\n",
    "ttest_x = test_x.apply(nltk.tokenize.word_tokenize)\n",
    "ttrain_x = train_x.apply(nltk.tokenize.word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that counts the number of lines in each data set as well as the maximum number of tokens in each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DEV:  4650\n",
      "Length of TRAIN:  37208\n",
      "Length of TEST:  4652\n",
      "Max Tokens in DEV:  61\n",
      "Max Tokens in TRAIN:  97\n",
      "Max Tokens in TEST:  66\n"
     ]
    }
   ],
   "source": [
    "dev_len = len(dev.index)\n",
    "test_len = len(test.index)\n",
    "train_len = len(train.index)\n",
    "\n",
    "print('Length of DEV: ', dev_len)\n",
    "print('Length of TRAIN: ', train_len)\n",
    "print('Length of TEST: ', test_len)\n",
    "\n",
    "dev_lens = [len(x) for x in tdev_x]\n",
    "dev_max = max(dev_lens)\n",
    "\n",
    "train_lens = [len(x) for x in ttrain_x]\n",
    "train_max = max(train_lens)\n",
    "\n",
    "test_lens = [len(x) for x in ttest_x]\n",
    "test_max = max(test_lens)\n",
    "\n",
    "print('Max Tokens in DEV: ', str(dev_max))\n",
    "print('Max Tokens in TRAIN: ',str(train_max))\n",
    "print('Max Tokens in TEST: ', str(test_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each data set's labels (gender) into numeric form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sorted(set(train_y))\n",
    "cat2index = {c:i for (i, c) in enumerate(categories)}\n",
    "\n",
    "tensor_ind_dev_y = torch.tensor([cat2index[category] for category in dev_y], dtype=torch.int64)\n",
    "tensor_ind_train_y = torch.tensor([cat2index[category] for category in train_y], dtype=torch.int64)\n",
    "tensor_ind_test_y = torch.tensor([cat2index[category] for category in test_y], dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a vocabulary consisting of the tokens that occur at least 5 times in the train set and output the size of your vocabulary.\n",
    "Include the unknown token and pad token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab 7113\n"
     ]
    }
   ],
   "source": [
    "min_freq = 5\n",
    "\n",
    "frequencies = collections.Counter(word for text in ttrain_x for word in text)\n",
    "vocab = sorted(frequencies.keys(), key=frequencies.get, reverse=True)\n",
    "while frequencies[vocab[-1]] < min_freq:\n",
    "    vocab.pop()\n",
    "vocab = ['<PAD>', '<UNK>'] + sorted(vocab)\n",
    "\n",
    "print(\"Vocab\", len(vocab))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary bag of words feature vectors for all data set texts using the vocabulary created above (include stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = sklearn.feature_extraction.text.CountVectorizer(vocabulary=vocab, binary=True, analyzer=lambda text: text, dtype=np.float32)\n",
    "encoder.fit(train_x)\n",
    "\n",
    "vdev_x = encoder.transform(tdev_x).toarray()\n",
    "vtrain_x = encoder.transform(ttrain_x).toarray()\n",
    "vtest_x = encoder.transform(ttest_x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data set of indexified token sequences for all texts using the vocabulary created above, making use of unknown tokens and pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {w:i for (i,w) in enumerate(vocab)}\n",
    "\n",
    "for i in range(len(tdev_x)):\n",
    "    for j in range(len(tdev_x[i])):\n",
    "        if tdev_x[i][j] not in word2index:\n",
    "            tdev_x[i][j] = '<UNK>'\n",
    "    tdev_x[i].extend(['<PAD>']*(dev_max - len(tdev_x[i])))\n",
    "    \n",
    "for i in range(len(ttrain_x)):\n",
    "    for j in range(len(ttrain_x[i])):\n",
    "        if ttrain_x[i][j] not in word2index:\n",
    "            ttrain_x[i][j] = '<UNK>'\n",
    "    ttrain_x[i].extend(['<PAD>']*(train_max - len(ttrain_x[i])))\n",
    "\n",
    "for i in range(len(ttest_x)):\n",
    "    for j in range(len(ttest_x[i])):\n",
    "        if ttest_x[i][j] not in word2index:\n",
    "            ttest_x[i][j] = '<UNK>'\n",
    "    ttest_x[i].extend(['<PAD>']*(test_max - len(ttest_x[i])))\n",
    "\n",
    "indexed_dev_x = torch.tensor([[word2index[word] for word in text] for text in tdev_x], dtype = torch.int64)\n",
    "indexed_test_x = torch.tensor([[word2index[word] for word in text] for text in ttest_x], dtype = torch.int64)\n",
    "indexed_train_x = torch.tensor([[word2index[word] for word in text] for text in ttrain_x], dtype = torch.int64)\n",
    "\n",
    "tensor_dev_len = torch.tensor(dev_lens, dtype=torch.int64)\n",
    "tensor_test_len = torch.tensor(test_lens, dtype=torch.int64)\n",
    "tensor_train_len = torch.tensor(train_lens, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that counts the percentage of tokens in each data set that are unknown tokens (not including pad tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK in DEV 2.32%\n",
      "UNK in TRAIN 1.31%\n",
      "UNK in TEST 2.19%\n"
     ]
    }
   ],
   "source": [
    "def unk(tokens):\n",
    "    total_tokens = sum([len(x) for x in tokens])    \n",
    "    unk_tokens = sum([1 if word == \"<UNK>\" else 0 for text in tokens for word in text])\n",
    "    return (unk_tokens/total_tokens)\n",
    "\n",
    "dev_unk = unk(tdev_x)\n",
    "train_unk = unk(ttrain_x)\n",
    "test_unk = unk(ttest_x)\n",
    "\n",
    "print(\"UNK in DEV {:.2%}\".format(dev_unk))\n",
    "print(\"UNK in TRAIN {:.2%}\".format(train_unk))\n",
    "print(\"UNK in TEST {:.2%}\".format(test_unk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression classification (20%)\n",
    "\n",
    "Write a linear regression classifier (single layer neural net) that is trained to classify the author gender from the bag of words vector of the text.\n",
    "You do not need to perform any hyperparameter tuning.\n",
    "Use L1 weight decay regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step error\n",
      "100 0.6532658338546753\n",
      "200 0.6431275606155396\n"
     ]
    }
   ],
   "source": [
    "class Linear(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_categories):\n",
    "        super().__init__()\n",
    "        self.w = torch.nn.Parameter(torch.zeros((vocab_size, num_categories), dtype=torch.float32, requires_grad=True))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((num_categories,), dtype=torch.float32, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x@self.w + self.b\n",
    "    \n",
    "linear = Linear(len(vocab), 2)\n",
    "linear.to('cpu')\n",
    "\n",
    "optimiser = torch.optim.Adam(linear.parameters())\n",
    "\n",
    "tensor_trainx = torch.tensor(vtrain_x, dtype=torch.float32)\n",
    "\n",
    "print('step', 'error')\n",
    "for step in range(1, 200+1):\n",
    "    optimiser.zero_grad()\n",
    "    output = linear(tensor_trainx)\n",
    "    error = torch.nn.functional.cross_entropy(output, tensor_ind_train_y) + linear.w.abs().mean()\n",
    "    error.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step%100 == 0:\n",
    "        print(step, error.detach().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy, precision, recall, and F1-score of this classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.38%\n",
      "Precision: 62.50%\n",
      "Recall: 61.91%\n",
      "F1-Score: 62.20%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(TP, TN, tot):\n",
    "    return (TP + TN)/tot\n",
    "\n",
    "def precision(TP, FP):\n",
    "    return TP/(TP + FP)\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP/(TP + FN)\n",
    "\n",
    "def f1(precision, recall):\n",
    "    num = precision * recall\n",
    "    denom = precision + recall\n",
    "    return 2 * (num/denom)\n",
    "\n",
    "ttest_x_vec = torch.tensor(vtest_x, dtype = torch.float32)\n",
    "targets = np.array(tensor_ind_test_y, np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    probability = torch.sigmoid(linear(ttest_x_vec))\n",
    "    output = probability.detach().numpy().argmax(axis=1)\n",
    "    \n",
    "tp = 0 \n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    if(targets[i] == output[i]):\n",
    "        if(targets[i] == 0):\n",
    "            tn += 1\n",
    "        else:\n",
    "            tp += 1\n",
    "    else:\n",
    "        if(targets[i] == 0):\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "accuracy = accuracy(tp, tn, len(targets))\n",
    "precision = precision(tp, fp)\n",
    "recall = recall(tp, fn)\n",
    "f1_score = f1(precision, recall)\n",
    "\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(accuracy))\n",
    "print('Precision: {:.2%}'.format(precision))\n",
    "print('Recall: {:.2%}'.format(recall))\n",
    "print('F1-Score: {:.2%}'.format(f1_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that shows the top 10 tokens that are the most important for determining the author gender according to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10\n",
      "1) -arv (19.25%)\n",
      "2) hakx (18.94%)\n",
      "3) gio (18.93%)\n",
      "4) jhayne (18.82%)\n",
      "5) dan (18.73%)\n",
      "6) venerable (18.60%)\n",
      "7) 1. (18.58%)\n",
      "8) killy (18.37%)\n",
      "9) managed (18.35%)\n",
      "10) -sane (18.35%)\n"
     ]
    }
   ],
   "source": [
    "temp = np.abs(linear.w.detach().numpy())\n",
    "\n",
    "category_index = 5\n",
    "weighted = sorted(zip(temp[:, :].tolist(), vocab), reverse=True)\n",
    "ten = []\n",
    "\n",
    "print('Top 10')\n",
    "for i, w in enumerate(weighted[:10]):\n",
    "    m = (w[0][0] + w[0][1]) / 2\n",
    "    mean = \"{:.2%}\".format(m)\n",
    "    print(i+1,\") \",w[1],\" (\",mean,\")\",sep=\"\")\n",
    "    ten.append(w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code that, for each data split and gender, shows the percentage of rows that include at least one of these important words (so 6 percentages in all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Occurance of 'HAKX'\n",
      "Dev Male: 0.06%\n",
      "Dev Female: 0.00%\n",
      "Train Male: 0.09%\n",
      "Train Female: 0.00%\n",
      "Test Male: 0.00%\n",
      "Test Female: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def percentage(text, data, gender):\n",
    "    temp = 0\n",
    "    total = len(data.index)\n",
    "    \n",
    "    for i, t in enumerate(data[\"text\"]):\n",
    "        t = t.lower()\n",
    "        \n",
    "        if(data[\"gender\"][i] == gender):\n",
    "            if(t.find(text) != -1):\n",
    "                temp += 1\n",
    "    \n",
    "    return temp/total\n",
    "\n",
    "#Chosen Random Word from Top 10\n",
    "top = 1\n",
    "\n",
    "dev_male = percentage(ten[top], dev, \"male\")\n",
    "dev_female = percentage(ten[top], dev, \"female\")\n",
    "\n",
    "train_male = percentage(ten[top], train, \"male\")\n",
    "train_female = percentage(ten[top], train, \"female\")\n",
    "\n",
    "test_male = percentage(ten[top], test, \"male\")\n",
    "test_female = percentage(ten[top], test, \"female\")\n",
    "\n",
    "print(\"Percentage Occurance of 'HAKX'\")\n",
    "\n",
    "print(\"Dev Male: {:.2%}\".format(dev_male))\n",
    "print(\"Dev Female: {:.2%}\".format(dev_female))\n",
    "\n",
    "print(\"Train Male: {:.2%}\".format(train_male))\n",
    "print(\"Train Female: {:.2%}\".format(train_female))\n",
    "\n",
    "print(\"Test Male: {:.2%}\".format(test_male))\n",
    "print(\"Test Female: {:.2%}\".format(test_female))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning classifier (50%)\n",
    "\n",
    "Perform hyperparameter tuning on a deep learning classifier (with a convolutional neural network or a recurrent neural network) that is trained to classify the author gender from the indexified sequences of the text.\n",
    "Using the dev set for evaluation.\n",
    "Output the best hyperparameters found and do not store the best trained model as you will be training it again in the next bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, categ_size, is_lstm, embedding_size, hidden_size, init_dev):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_lstm = is_lstm\n",
    "        \n",
    "        self.embedding_matrix = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (vocab_size, embedding_size)), dtype=torch.float32))\n",
    "        self.s0 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (hidden_size,)), dtype=torch.float32))\n",
    "        \n",
    "        if is_lstm:\n",
    "            self.lstm = torch.nn.LSTMCell(embedding_size, hidden_size)\n",
    "            self.c0 = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (hidden_size,)), dtype=torch.float32))\n",
    "\n",
    "        else:\n",
    "            self.gru = torch.nn.GRUCell(embedding_size, hidden_size)   \n",
    "\n",
    "        self.w = torch.nn.Parameter(torch.tensor(np.random.normal(0.0, init_dev, (hidden_size, categ_size)), dtype=torch.float32))\n",
    "        self.b = torch.nn.Parameter(torch.zeros((categ_size,), dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, text_lens):\n",
    "        batch_size = x.shape[0]\n",
    "        time_steps = x.shape[1]\n",
    "\n",
    "        embedded = self.embedding_matrix[x]\n",
    "        state = self.s0.unsqueeze(0).tile((batch_size, 1))\n",
    "        if self.is_lstm:\n",
    "            c = self.c0.unsqueeze(0).tile((batch_size, 1))\n",
    "        for t in range(time_steps):\n",
    "            mask = (t < text_lens).unsqueeze(1).tile((1, self.hidden_size))\n",
    "            if self.is_lstm:\n",
    "                (next_state, c) = self.lstm(embedded[:, t, :], (state, c))\n",
    "            else:\n",
    "                next_state = self.gru(embedded[:, t, :], state)\n",
    "            state = torch.where(mask, next_state, state)\n",
    "        return state@self.w + self.b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyperparameters found in the previous bit to train the classifier, this time outputting a graph showing the dev set accuracy after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, epochs):\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    xy = make_interp_spline(x, y)\n",
    "    X_ = np.linspace(x.min(), x.max(), 500)\n",
    "    Y_ = xy(X_)\n",
    "    \n",
    "    plt.plot(X_, Y_, color='blue')\n",
    "    \n",
    "    plt.title('Dev Accuracy')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    \n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.xlim([0,epochs])\n",
    "    \n",
    "    y = np.delete(y,0)\n",
    "    \n",
    "    min_y = min(y)\n",
    "    max_y = max(y)\n",
    "    min_y -= 5\n",
    "    max_y += 5\n",
    "    \n",
    "    if min_y < 0:\n",
    "        min_y = 0\n",
    "    if max_y > 100:\n",
    "        max_y = 100\n",
    "        \n",
    "    plt.ylim([min_y, max_y])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy, precision, recall, and F1-score of this classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-8e56a64e29cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# store accuracy in results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|       Accuracy      |'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "def accuracy(TP, TN, tot):\n",
    "    return (TP + TN)/tot\n",
    "\n",
    "def precision(TP, FP):\n",
    "    return TP/(TP + FP)\n",
    "\n",
    "def recall(TP, FN):\n",
    "    return TP/(TP + FN)\n",
    "\n",
    "def f1(precision, recall):\n",
    "    num = precision * recall\n",
    "    denom = precision + recall\n",
    "    return 2 * (num/denom)\n",
    "\n",
    "ttest_x_vec = torch.tensor(vtest_x, dtype = torch.float32)\n",
    "targets = np.array(tensor_ind_test_y, np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    probability = torch.sigmoid(recurrent(ttest_x_vec))\n",
    "    output = probability.detach().numpy().argmax(axis=1)\n",
    "    \n",
    "tp = 0 \n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    if(targets[i] == output[i]):\n",
    "        if(targets[i] == 0):\n",
    "            tn += 1\n",
    "        else:\n",
    "            tp += 1\n",
    "    else:\n",
    "        if(targets[i] == 0):\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "            \n",
    "accuracy = accuracy(tp, tn, len(targets))\n",
    "precision = precision(tp, fp)\n",
    "recall = recall(tp, fn)\n",
    "f1_score = f1(precision, recall)\n",
    "\n",
    "\n",
    "print('Accuracy: {:.2%}'.format(accuracy))\n",
    "print('Precision: {:.2%}'.format(precision))\n",
    "print('Recall: {:.2%}'.format(recall))\n",
    "print('F1-Score: {:.2%}'.format(f1_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a confusion matrix of the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output 5 examples of correctly classified text for each gender and 5 examples of incorrectly classified text for each gender (so 20 text examples in total), all of which must be from the test set.\n",
    "This is assuming that you have at least 5 instances of each group.\n",
    "If you have less, then show whatever is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the list of important tokens determined previously (from the logistic regression classifier)?\n",
    "Write code that takes all the texts in the test set that have at least one of the important tokens and shows the percentage of these texts that were correctly classified.\n",
    "Similarly, take all the texts that don't have any of the important tokens and show the percentage of these texts that were correctly classified (so 2 percentages in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (10%)\n",
    "\n",
    "Write, in less than 300 words, your interpretation of the results and how you think the model could perform better.\n",
    "You should talk about things like overfitting/underfitting and whether the model is learning anything deep about how the different genders write or if it's just basing everything on the words used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75ee2b71ad44bf9ef4e9bee896f68ffbc764a6a2c6d1f57c86c48f99ffc25ca8"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
